# Configuration for Persistent Graph Memory System

# --- File Paths ---
base_memory_path: "memory_sets"
data_dir: "memory_data"
workspace_dir: "Workspace" # Subdirectory within data_dir
calendar_file: "calendar.jsonl" # Filename for calendar within workspace

# Default personality folder name (used if none selected)
default_personality: "default"

# --- Models & API ---
embedding_model: "all-MiniLM-L6-v2"
tokenizer_name: "google/gemma-7b-it"
kobold_api_url: "http://localhost:5001/api/v1/generate" # Base URL for generate API
kobold_chat_api_url: "http://localhost:5001/v1/chat/completions" # Base URL for OpenAI-compatible chat API

# --- LLM Model Configuration ---
# Define settings for different LLM tasks.
# 'api_type' can be 'generate' (uses kobold_api_url) or 'chat_completions' (uses kobold_chat_api_url).
# 'model_name' is often ignored by KoboldCpp but good practice.
# Other parameters are standard generation settings.
llm_models:
  # --- Main Chat ---
  main_chat_text:
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 512
    temperature: 0.7
    top_p: 0.95
    top_k: 64
    min_p: 0.0
  main_chat_multimodal:
    api_type: "chat_completions"
    model_name: "koboldcpp-multimodal" # Or specific multimodal model name
    max_tokens: 512 # Note: Parameter name is max_tokens for chat API
    temperature: 0.7
    top_p: 0.9
  # --- Analysis Tasks (Generally need low temperature, specific max lengths) ---
  # --- DEPRECATED: Use workspace_planning instead ---
  action_analysis: # Keep entry but maybe mark as unused or lower resources
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 10 # Reduce length significantly if kept
    temperature: 0.1
    top_p: 0.95
    top_k: 10
    min_p: 0.0
  memory_modification_analysis:
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 512 # Allow longer for complex args
    temperature: 0.1
    top_p: 0.95
    top_k: 40
    min_p: 0.0
  memory_modification_analysis:
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 150
    temperature: 0.2
    top_p: 0.95
    top_k: 40
    min_p: 0.0
  query_type_classification:
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 10
    temperature: 0.1
    top_p: 0.95
    top_k: 10 # Small K for classification
    min_p: 0.0
  intention_analysis:
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 200
    temperature: 0.15
    top_p: 0.95
    top_k: 40
    min_p: 0.0
  # --- Workspace File Summarization Task (for context) ---
  workspace_file_summary:
    api_type: "generate"
    model_name: "koboldcpp-default" # Use a fast model if possible
    max_length: 60 # Keep summaries very short (1-2 sentences)
    temperature: 0.3 # Low temp for factual summary
    top_p: 0.95
    top_k: 40
    min_p: 0.0
  # --- Workspace File Consolidation Task ---
  workspace_file_consolidation:
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 2048 # Needs potentially large context/output
    temperature: 0.5 # Balance creativity and coherence
    top_p: 0.95
    top_k: 50
    min_p: 0.0
  # --- Workspace Planning Task ---
  workspace_planning:
    api_type: "generate" # Or chat_completions if model works better
    model_name: "koboldcpp-default"
    max_length: 768 # Allow more tokens for potentially complex plans/content
    temperature: 0.2 # Low temp for structured output
    top_p: 0.95
    top_k: 40
    min_p: 0.0
  # --- Workspace Planning Task ---
  workspace_planning:
    api_type: "generate" # Or chat_completions if model works better
    model_name: "koboldcpp-default"
    max_length: 768 # Allow more tokens for potentially complex plans/content
    temperature: 0.2 # Low temp for structured output
    top_p: 0.95
    top_k: 40
    min_p: 0.0
  # --- Consolidation Tasks ---
  consolidation_summary:
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 150
    temperature: 0.5
    top_p: 0.95
    top_k: 60
    min_p: 0.0
  consolidation_concept:
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 100
    temperature: 0.3
    top_p: 0.95
    top_k: 50
    min_p: 0.0
  consolidation_relation: # Rich relations
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 400
    temperature: 0.15
    top_p: 0.95
    top_k: 40
    min_p: 0.0
  consolidation_causal:
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 300
    temperature: 0.15
    top_p: 0.95
    top_k: 40
    min_p: 0.0
  consolidation_analogy:
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 200
    temperature: 0.2
    top_p: 0.95
    top_k: 50
    min_p: 0.0
  consolidation_associative_v1: # Old V1 prompt
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 100
    temperature: 0.4
    top_p: 0.95
    top_k: 50
    min_p: 0.0
  consolidation_hierarchy_v1: # Old V1 prompt
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 100
    temperature: 0.4
    top_p: 0.95
    top_k: 50
    min_p: 0.0
  # --- Other Generation Tasks ---
  asm_generation:
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 400
    temperature: 0.6
    top_p: 0.95
    top_k: 60
    min_p: 0.0
  file_content_generation:
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 1024 # Allow more for file content
    temperature: 0.7
    top_p: 0.95
    top_k: 60
    min_p: 0.0
  drive_analysis_short_term:
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 150
    temperature: 0.3
    top_p: 0.95
    top_k: 40
    min_p: 0.0
  drive_analysis_long_term:
    api_type: "generate"
    model_name: "koboldcpp-default"
    max_length: 200
    temperature: 0.2
    top_p: 0.95
    top_k: 40
    min_p: 0.0
  # --- Re-Greeting Generation ---
  re_greeting_generation:
    api_type: "generate" # Or chat_completions if preferred
    model_name: "koboldcpp-default"
    max_length: 100 # Greeting should be relatively short
    temperature: 0.75 # Slightly higher temp for more natural greeting
    top_p: 0.95
    top_k: 60
    min_p: 0.0


# --- Feature Flags ---
features:
  enable_saliency: true # Enable/disable saliency calculations
  enable_emotion_analysis: true # Enable/disable basic emotion analysis during consolidation
  enable_forgetting: true # Enable/disable nuanced forgetting
  enable_rich_associations: true # Enable/disable LLM-based rich association extraction (Phase 1.1+)

# --- Activation Spreading Parameters ---
activation:
  initial: 0.7 # Reduced from 1.0
  threshold: 0.18 # Increased from 0.1
  node_decay_rate: 0.02
  edge_decay_rate: 0.01
  spreading_depth: 3
  max_initial_nodes: 7
  propagation_factor_base: 0.65 # Reduced from 0.7
  propagation_factors:
    TEMPORAL_fwd: 1.0
    TEMPORAL_bwd: 0.8
    SUMMARY_OF_fwd: 1.2
    SUMMARY_OF_bwd: 0.4
    MENTIONS_CONCEPT_fwd: 1.0
    MENTIONS_CONCEPT_bwd: 0.9
    ASSOCIATIVE: 0.8
    HIERARCHICAL_fwd: 1.1
    HIERARCHICAL_bwd: 0.5
    # --- NEW Relation Types (Add factors for types used in consolidation) ---
    CAUSES: 1.1             # Cause->Effect might be strong forward
    PART_OF: 1.0            # Part->Whole might be standard
    HAS_PROPERTY: 0.9       # Entity->Property might be slightly weaker
    ENABLES: 1.0
    PREVENTS: 1.0           # Contradiction/Prevention might warrant specific handling later
    CONTRADICTS: 1.0
    SUPPORTS: 1.0
    EXAMPLE_OF: 0.9
    MEASURES: 0.9
    LOCATION_OF: 0.9
    ANALOGY: 0.8            # Analogy might be similar to associative
    INFERRED_RELATED_TO: 0.6 # Inferred links should likely be weaker
    SPACY_REL: 0.7          # Generic factor for spaCy-derived relations
    UNKNOWN: 0.5            # Fallback for truly unknown types
  # --- Context Focus ---
  context_focus_boost: 0.15 # Multiplicative boost to initial activation for recently mentioned concepts/turns
  # --- Interference Simulation ---
  interference:
    enable: true
    check_threshold: 0.15 # Activation level above which nodes trigger interference checks
    similarity_threshold: 0.25 # Max FAISS L2 distance for neighbors to interfere (Lower = more similar)
    penalty_factor: 0.90 # Multiplicative factor applied to activation of interfered nodes (e.g., 0.9 reduces activation by 10%)
    max_neighbors_check: 5 # How many nearest neighbors to check for interference
  # --- Emotional Context Bias ---
  emotional_context:
    enable: true
    max_distance: 1.414 # Approx max Euclidean distance in V/A space (-1..1, 0..1 -> sqrt(2^2+1^2) is too high, use sqrt(1^2+1^2))
    boost_factor: 0.1 # Max *additive* boost for emotionally close nodes (applied to act_pass)
    penalty_factor: 0.05 # Max *subtractive* penalty for emotionally distant nodes (applied to act_pass)
    # --- Emotional Reconsolidation on Recall ---
    reconsolidation_enable: true
    reconsolidation_threshold: 0.5 # Min V/A distance to trigger reconsolidation adjustment
    reconsolidation_factor: 0.05 # How much to nudge node emotion towards current mood (0 to 1)
  # --- Short-Term Priming ---
  priming_boost_factor: 1.5 # Multiplicative boost for immediate previous turn(s)

# --- Prompting & Context Configuration ---
prompting:
  max_context_tokens: 8192 # Corrected the float value to integer
  context_headroom: 250
  memory_budget_ratio: 0.45
  history_budget_ratio: 0.55
  re_greeting_threshold_hours: 3 # Hours since last interaction to trigger re-greeting
  max_files_to_summarize_in_context: 5 # Max files to read & summarize for prompt context

# --- Consolidation Parameters ---
consolidation:
  trigger_interaction_count: 0 # How often to run consolidation automatically (0 to disable)
  turn_count: 10 # How many turns to process when consolidation runs
  min_nodes: 5 # Minimum suitable nodes required to proceed
  concept_similarity_threshold: 0.3
  # --- Relation Extraction ---
  target_relation_types: # List of relation types the LLM should try to extract
    - "CAUSES"
    - "PART_OF"
    - "HAS_PROPERTY"
    - "RELATED_TO"
    - "IS_A"
    - "ENABLES"
    - "PREVENTS"
    - "CONTRADICTS"
    - "SUPPORTS"
    - "EXAMPLE_OF"
    - "MEASURES"
    - "LOCATION_OF"
  enable_causal_chains: true # Enable/disable causal chain extraction during consolidation
  enable_analogies: true # Enable/disable analogy extraction during consolidation
  # --- Second-Order Inference ---
  inference:
    enable: true
    max_depth: 2 # Fixed at 2 for V1 generic relatedness
    strength_factor: 0.3 # Multiplier for inferred edge strength (based on path strengths)

# --- Emotion Analysis ---
emotion_analysis:
  default_valence: 0.0 # Neutral
  default_arousal: 0.1 # Slightly above baseline minimum arousal

# --- Forget Topic ---
forget_topic:
  similarity_k: 15

# --- Nuanced Forgetting ---
forgetting:
  trigger_interaction_count: 20 # Reverted from 5 back to a higher value for normal operation
  score_threshold: 0.7 # Score above which nodes get archived
  candidate_min_age_hours: 24 # Only consider nodes older than this for strength reduction check
  candidate_min_activation: 0.05 # Only consider nodes with low activation for strength reduction check
  # --- Forgettability Score Weights (used to calculate strength reduction) ---
  weights:
    # Factors increasing forgettability score (leading to more strength reduction):
    recency_factor: 0.4      # Increased from 0.3
    activation_factor: 0.3   # Increased from 0.2
    node_type_factor: 0.1    # e.g., 'turn' nodes are more forgettable than others
    # Factors decreasing forgettability (Resistance):
    saliency_factor: 0.15    # Decreased from 0.2
    emotion_factor: 0.1      # Higher emotional intensity = less forgettable (Placeholder)
    connectivity_factor: 0.05 # Decreased from 0.1
    access_count_factor: 0.05 # Added small weight
    emotion_magnitude_resistance_factor: 0.1 # NEW: How much emotion magnitude reduces forgettability (0=none)
    # --- Recency Decay Constant (Higher value = faster initial forgetting score component) ---
    recency_decay_constant: 0.000005 # Example value (adjust based on desired time scale, e.g., for seconds)
  # --- Decay Resistance (Lower factor = Slower decay) ---
  # Multiplies the final forgettability score. Core memories should have very low factors.
  decay_resistance:
    turn: 1.0      # Standard decay rate for conversational turns
    summary: 0.1   # Summaries decay 10x slower
    concept: 0.05  # Concepts decay 20x slower
    intention: 0.4 # Intentions decay slightly slower than before (was 0.5)
    default: 1.0   # Default for unknown types

# --- Memory Strength Parameters ---
memory_strength:
  initial_value: 1.0 # Starting strength for new nodes
  decay_rate: 0.15 # Factor applied to forgettability score for strength reduction
  purge_threshold: 0.01 # Strength below which nodes are eligible for permanent deletion by purge_weak_nodes
  # purge_min_age_days: 60 # REMOVED: Purge is now based only on strength threshold

# --- GUI Keywords --- # Moved from bottom for better grouping
modification_keywords:
  - "forget"
  - "delete"
  - "remove"
  - "edit"
  - "change"
  - "correct"
  - "update"


# --- GUI Style Configuration ---
# (Keep existing style config)
gui_style:
  bubbles:
    max_width_percent: 75
    side_margin_px: 50
    edge_margin_px: 5
    min_width_px: 400
    border_radius_px: 18
    internal_padding_top_bottom_px: 6
    internal_padding_left_right_px: 10
    timestamp_color_user: "#E0E0E0"
    timestamp_color_ai: "#B0B0B0"
    thumbnail_max_width_px: 200
  input_field:
    border_radius_px: 15
    padding_px: 8
  buttons:
    border_radius_px: 15
    padding_vertical_px: 8
    padding_horizontal_px: 16

# --- Subconscious Drives ---
subconscious_drives:
  enabled: true # Master switch for the drive system

  # --- Core Drive Definitions & Baselines ---
  # Define the core drives and their default baseline levels (0.0 = neutral).
  # These represent the AI's inherent tendencies before long-term learning.
  base_drives:
    Connection: 0.1 # Base desire for positive social interaction.
    Safety: 0.2     # Base desire for security, stability, predictability.
    Understanding: 0.1 # Base drive to learn, reduce uncertainty.
    Novelty: 0.05   # Base drive for new experiences, stimulation.
    Control: 0.1    # Base drive for agency, competence, influence.

  # --- Short-Term Drive Dynamics ---
  # Short-term state fluctuates based on recent events and decays towards a dynamic baseline.
  short_term_update_interval_interactions: 10 # How often (in user/AI turns) to run LLM analysis for short-term updates (0=disable LLM analysis).
  short_term_decay_rate: 0.05 # How quickly short-term activation returns towards dynamic baseline per update cycle (0=no decay). Higher = faster return to baseline.

  # --- Heuristic Short-Term Adjustments ---
  # Direct adjustments applied immediately based on specific events. Values are added/subtracted from the short-term drive level.
  heuristic_adjustment_factors:
    action_success_control: 0.05   # Increase Control on successful action execution.
    action_fail_control: -0.08     # Decrease Control on failed action execution.
    saliency_increase_connection: 0.03 # Increase Connection slightly on [+S] feedback.
    saliency_decrease_connection: -0.02 # Decrease Connection slightly on [-S] feedback.
    # Add more heuristics here (e.g., keyword triggers) if desired later.

  # --- LLM-Based Short-Term Adjustments ---
  # Factors applied based on LLM analysis ("satisfied"/"frustrated") during the update interval.
  llm_satisfaction_factor: 0.1 # How much 'satisfied' status reduces drive activation (moves towards baseline). Applied multiplicatively to the deviation above baseline.
  llm_frustration_factor: 0.15 # How much 'frustrated' status increases drive activation (moves away from baseline). Applied additively.

  # --- Long-Term Drive Dynamics ---
  # Long-term state reflects persistent tendencies, updated less frequently based on ASM analysis.
  long_term_update_interval_consolidations: 5 # How many consolidations before running LLM analysis for long-term updates (0=disable).
  long_term_adjustment_factor: 0.05 # How much LLM analysis nudges long-term drives per update (0-1 scale).
  long_term_influence_on_baseline: 1.0 # Multiplier for how much long-term drives shift the short-term baseline (0=none, 1=direct addition).

  # --- Mood Influence (How drive deviations affect V/A used for retrieval bias) ---
  mood_influence:
    # Factors multiplied by the drive's current *deviation* (short_term_activation - dynamic_baseline)
    # to calculate adjustments to the base Valence/Arousal mood state.
    # Positive deviation = drive level is *higher* than baseline (need potentially met/overshot).
    # Negative deviation = drive level is *lower* than baseline (need potentially unmet).
    valence_factors: # How deviation affects Pleasantness (-1 to +1)
      Connection: 0.15  # Positive deviation (met) -> more pleasant. Negative (unmet) -> less pleasant.
      Safety: -0.25     # Positive deviation (safe) -> slightly less pleasant (complacent?). Negative (unsafe) -> much less pleasant.
      Understanding: 0.1 # Positive deviation (understanding) -> more pleasant. Negative (confused) -> less pleasant.
      Novelty: 0.1     # Positive deviation (stimulated) -> more pleasant. Negative (bored) -> less pleasant. (Changed sign from 0.05)
      Control: 0.1     # Positive deviation (in control) -> more pleasant. Negative (loss of control) -> less pleasant.
    arousal_factors: # How deviation affects Activation/Energy (0 to 1)
      Connection: 0.1  # Both high satisfaction and high frustration might increase arousal slightly. (Increased from 0.05)
      Safety: -0.3     # Positive deviation (safe) -> decreases arousal (calm). Negative (unsafe) -> increases arousal (anxiety). (Correct)
      Understanding: 0.1 # Positive deviation (engaged) -> increases arousal. Negative (confused) -> increases arousal (frustration). (Increased from 0.05)
      Novelty: 0.25    # Positive deviation (excited) -> increases arousal. Negative (bored) -> decreases arousal. (Increased from 0.2)
      Control: -0.15   # Positive deviation (competent) -> decreases arousal (calm control). Negative (loss of control) -> increases arousal (agitation). (Changed sign from 0.15)
    max_mood_adjustment: 0.3 # Maximum absolute change to V or A from the sum of all drive influences in one step.

# --- Feedback System ---
feedback_system:
  enable: true # Enable/disable user feedback processing
  # --- Saliency Influence ---
  # Additive adjustments to saliency score based on feedback
  saliency_upvote_boost: 0.1
  saliency_downvote_penalty: 0.15 # Penalty is subtracted

# --- Saliency Calculation ---
saliency:
  initial_scores: # Base scores by node type
    turn: 0.4
    summary: 0.7
    concept: 0.8
    intention: 0.75 # NEW: Saliency for intention nodes
    default: 0.5
  emotion_influence_factor: 0.2 # How much default arousal boosts initial saliency
  activation_influence: 0.1 # How much saliency boosts activation spreading
  guaranteed_saliency_threshold: 0.85 # Nodes above this saliency are always included in retrieval if reached
  recall_boost_factor: 0.05 # Additive boost to saliency score upon successful recall (clamped 0-1)
  feedback_factor: 0.15 # Multiplicative factor for user feedback adjustment (e.g., 1.15 to increase, 1/1.15 to decrease)
